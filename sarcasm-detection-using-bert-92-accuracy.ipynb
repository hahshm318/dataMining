{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sarcasm Detection using the Pre-Trained BERT model from Transformers ","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-27T13:46:34.059532Z","iopub.execute_input":"2023-04-27T13:46:34.059891Z","iopub.status.idle":"2023-04-27T13:46:34.074047Z","shell.execute_reply.started":"2023-04-27T13:46:34.059860Z","shell.execute_reply":"2023-04-27T13:46:34.072662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom transformers import TFBertModel, BertTokenizer\nfrom sklearn.model_selection import train_test_split\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T13:46:34.076731Z","iopub.execute_input":"2023-04-27T13:46:34.077501Z","iopub.status.idle":"2023-04-27T13:46:34.083856Z","shell.execute_reply.started":"2023-04-27T13:46:34.077350Z","shell.execute_reply":"2023-04-27T13:46:34.082607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_json(\"../input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\nlabels = data.is_sarcastic.values\nsentences = data.headline.values\ndata.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2023-04-27T13:46:34.085594Z","iopub.execute_input":"2023-04-27T13:46:34.086599Z","iopub.status.idle":"2023-04-27T13:46:34.242115Z","shell.execute_reply.started":"2023-04-27T13:46:34.086551Z","shell.execute_reply":"2023-04-27T13:46:34.241189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tokenizing all the sentences using pre-trained BERT model from Tranformers**","metadata":{}},{"cell_type":"code","source":"PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME,do_lower_case = True)\n\ndef encoder(sentences):\n  ids = []\n  for sentence in sentences:\n    encoding = tokenizer.encode_plus(\n    sentence,\n    max_length=16,\n    truncation = True,\n    add_special_tokens=True,\n    return_token_type_ids=False,\n    pad_to_max_length=True,\n    return_attention_mask=False)\n    ids.append(encoding['input_ids'])\n  return ids\n\n#Train test split\ntrain_sents,test_sents, train_labels, test_labels  = train_test_split(sentences,labels,test_size=0.15)\n\ntrain_ids = encoder(train_sents)\ntest_ids = encoder(test_sents) ","metadata":{"execution":{"iopub.status.busy":"2023-04-27T13:46:34.244455Z","iopub.execute_input":"2023-04-27T13:46:34.244734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Converting arrays to tensors","metadata":{}},{"cell_type":"code","source":"train_ids = tf.convert_to_tensor(train_ids)\ntest_ids = tf.convert_to_tensor(test_ids)\ntest_labels = tf.convert_to_tensor(test_labels)\ntrain_labels = tf.convert_to_tensor(train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building Model from transformers using pre-trained bert-base-uncased (12-layer, 768-hidden, 12-heads, 110M parameters.)","metadata":{}},{"cell_type":"code","source":"bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')\ninput_word_ids = tf.keras.Input(shape=(16,), dtype=tf.int32, name=\"input_word_ids\")  \nembedding = bert_encoder([input_word_ids])\ndense = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(embedding[0])\ndense = tf.keras.layers.Dense(128, activation='relu')(dense)\ndense = tf.keras.layers.Dropout(0.2)(dense)   \noutput = tf.keras.layers.Dense(1, activation='sigmoid')(dense)    \n\nmodel = tf.keras.Model(inputs=[input_word_ids], outputs=output)  \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x = train_ids, y = train_labels, epochs = 3, verbose = 1, batch_size = 32, validation_data = (test_ids, test_labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}